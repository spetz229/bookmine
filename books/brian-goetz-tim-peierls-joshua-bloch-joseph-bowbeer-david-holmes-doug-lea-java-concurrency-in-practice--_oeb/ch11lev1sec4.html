---
layout: page
title: "Java Concurrency in Practice"
prev: ch11lev1sec3.html
next: ch11lev1sec5.html
book_path: books/brian-goetz-tim-peierls-joshua-bloch-joseph-bowbeer-david-holmes-doug-lea-java-concurrency-in-practice--_oeb/
---
{% include JB/setup %}
{% raw %}
<div>


<a name="ch11lev1sec4" class="calibre18" id="ch11lev1sec4"></a>
<h3 id="title-IDAM31XE" class="docSection1Title">11.4. Reducing Lock Contention</h3>
<p class="docText1">We've seen that serialization hurts scalability and that context switches hurt performance. Contended locking causes both, so reducing lock contention can improve both performance and scalability.</p>
<p class="docText1">Access to resources guarded by an exclusive lock is serializedonly one thread at a time may access it. Of course, we use locks for good reasons, such as preventing data corruption, but this safety comes at a price. Persistent contention for a lock limits scalability.</p>
<a name="ch11sb06" class="calibre18" id="ch11sb06"></a><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<p class="docText1">The principal threat to scalability in concurrent applications is the exclusive resource lock.</p>
</td></tr></table></p><p class="calibre1"> </p>
<p class="docText1">Two factors influence the likelihood of contention for a lock: how often that lock is requested and how long it is held once acquired.<sup class="docFootnote"><a class="calibre2" href="#ch11fn07">[7]</a></sup> If the product of these factors is sufficiently small, then most attempts to acquire the lock will be uncontended, and lock contention will not pose a significant scalability impediment. If, however, the lock is in sufficiently high demand, threads will block waiting for it; in the extreme case, processors will sit idle even though there is plenty of work to do.</p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn07" class="calibre18" id="ch11fn07">[7]</a></sup> This is a corollary of <span class="docEmphasis">Little's law</span>, a result from queueing theory that says "the average number of customers in a stable system is equal to their average arrival rate multiplied by their average time in the system". (<a class="calibre2" href="bib01.html#biblio01_023">Little, 1961</a>)</p></blockquote>
<a name="ch11sb07" class="calibre18" id="ch11sb07"></a><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<p class="docText1"><a name="iddle2078" class="calibre18" id="iddle2078"></a><a name="iddle2548" class="calibre18" id="iddle2548"></a><a name="iddle2549" class="calibre18" id="iddle2549"></a><a name="iddle3080" class="calibre18" id="iddle3080"></a><a name="iddle3145" class="calibre18" id="iddle3145"></a><a name="iddle3290" class="calibre18" id="iddle3290"></a><a name="iddle3291" class="calibre18" id="iddle3291"></a><a name="iddle3292" class="calibre18" id="iddle3292"></a><a name="iddle4071" class="calibre18" id="iddle4071"></a><a name="iddle4109" class="calibre18" id="iddle4109"></a><a name="iddle4110" class="calibre18" id="iddle4110"></a><a name="iddle4907" class="calibre18" id="iddle4907"></a><a name="iddle4908" class="calibre18" id="iddle4908"></a>There are three ways to reduce lock contention:</p>
<ul class="calibre15"><li class="calibre16"><p class="docText1">Reduce the duration for which locks are held;</p></li><li class="calibre16"><p class="docText1">Reduce the frequency with which locks are requested; or</p></li><li class="calibre16"><p class="docText1">Replace exclusive locks with coordination mechanisms that permit greater concurrency.</p></li></ul>
</td></tr></table></p><p class="calibre1"> </p>
<a name="ch11lev2sec8" class="calibre18" id="ch11lev2sec8"></a>
<h4 id="title-IDA3DIGN" class="docSection2Title">11.4.1. Narrowing Lock Scope ("Get in, Get Out")</h4>
<p class="docText1">An effective way to reduce the likelihood of contention is to hold locks as briefly as possible. This can be done by moving code that doesn't require the lock out of <tt class="calibre25">synchronized</tt> blocks, especially for expensive operations and potentially blocking operations such as I/O.</p>
<p class="docText1">It is easy to see how holding a "hot" lock for too long can limit scalability; we saw an example of this in <tt class="calibre25">SynchronizedFactorizer</tt> in <a class="calibre2" href="ch02_split_000.html#ch02">Chapter 2</a>. If an operation holds a lock for 2 milliseconds and every operation requires that lock, throughput can be no greater than 500 operations per second, no matter how many processors are available. Reducing the time the lock is held to 1 millisecond improves the lock-induced throughput limit to 1000 operations per second.<sup class="docFootnote"><a class="calibre2" href="#ch11fn08">[8]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn08" class="calibre18" id="ch11fn08">[8]</a></sup> Actually, this calculation <span class="docEmphasis">understates</span> the cost of holding locks for too long because it doesn't take into account the context switch overhead generated by increased lock contention.</p></blockquote>
<p class="docText1"><tt class="calibre25">AttributeStore</tt> in <a class="calibre2" href="#ch11list04">Listing 11.4</a> shows an example of holding a lock longer than necessary. The <tt class="calibre25">userLocationMatches</tt> method looks up the user's location in a <tt class="calibre25">Map</tt> and uses regular expression matching to see if the resulting value matches the supplied pattern. The entire <tt class="calibre25">userLocationMatches</tt> method is <tt class="calibre25">synchronized</tt>, but the only portion of the code that actually needs the lock is the call to <tt class="calibre25">Map.get</tt>.</p>
<a name="ch11list04" class="calibre18" id="ch11list04"></a><h5 id="title-IDAUFIGN" class="docExampleTitle">Listing 11.4. Holding a Lock Longer than Necessary.</h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<img border="0" alt="" src="face1.jpg" class="calibre31"/>
<pre class="calibre30">@ThreadSafe
public class AttributeStore {
    @GuardedBy("this") private final Map&lt;String, String&gt;
            attributes = new HashMap&lt;String, String&gt;();

    public <span class="docEmphStrong">synchronized</span>  boolean userLocationMatches(String name,
                                                     String regexp) {
        String key = "users." + name + ".location";
        String location = attributes.get(key);
        if (location == null)
            return false;
        else
            return Pattern.matches(regexp, location);
    }
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<p class="docText1"><a name="iddle1081" class="calibre18" id="iddle1081"></a><a name="iddle1161" class="calibre18" id="iddle1161"></a><a name="iddle1162" class="calibre18" id="iddle1162"></a><a name="iddle1860" class="calibre18" id="iddle1860"></a><a name="iddle1861" class="calibre18" id="iddle1861"></a><a name="iddle2082" class="calibre18" id="iddle2082"></a><a name="iddle4491" class="calibre18" id="iddle4491"></a><a name="iddle4735" class="calibre18" id="iddle4735"></a><tt class="calibre25">BetterAttributeStore</tt> in <a class="calibre2" href="#ch11list05">Listing 11.5</a> rewrites <tt class="calibre25">AttributeStore</tt> to reduce significantly the lock duration. The first step is to construct the <tt class="calibre25">Map</tt> key associated with the user's location, a string of the form <tt class="calibre25">users.</tt><span class="docEmphasis">name</span><tt class="calibre25">.location</tt>. This entails instantiating a <tt class="calibre25">StringBuilder</tt> object, appending several strings to it, and instantiating the result as a <tt class="calibre25">String</tt>. After the location has been retrieved, the regular expression is matched against the resulting location string. Because constructing the key string and processing the regular expression do not access shared state, they need not be executed with the lock held. <tt class="calibre25">BetterAttributeStore</tt> factors these steps out of the <tt class="calibre25">synchronized</tt> block, thus reducing the time the lock is held.</p>
<a name="ch11list05" class="calibre18" id="ch11list05"></a><h5 id="title-IDAGSQCN" class="docExampleTitle">Listing 11.5. Reducing Lock Duration.</h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<pre class="calibre30">@ThreadSafe
public class BetterAttributeStore {
    @GuardedBy("this") private final Map&lt;String, String&gt;
            attributes = new HashMap&lt;String, String&gt;();

    public boolean userLocationMatches(String name, String regexp) {
        String key = "users." + name + ".location";
        String location;
        <span class="docEmphStrong">synchronized (this)</span> {
            location = attributes.get(key);
        }
        if (location == null)
            return false;
        else
            return Pattern.matches(regexp, location);
    }
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<p class="docText1">Reducing the scope of the lock in <tt class="calibre25">userLocationMatches</tt> substantially reduces the number of instructions that are executed with the lock held. By Amdahl's law, this removes an impediment to scalability because the amount of serialized code is reduced.</p>
<p class="docText1">Because <tt class="calibre25">AttributeStore</tt> has only one state variable, <tt class="calibre25">attributes</tt>, we can improve it further by the technique of <span class="docEmphasis">delegating thread safety</span> (<a class="calibre2" href="ch04lev1sec3.html#ch04lev1sec3">Section 4.3</a>). By replacing <tt class="calibre25">attributes</tt> with a thread-safe <tt class="calibre25">Map</tt> (a <tt class="calibre25">Hashtable</tt>, <tt class="calibre25">synchronizedMap</tt>, or <tt class="calibre25">ConcurrentHashMap</tt>), <tt class="calibre25">AttributeStore</tt> can delegate all its thread safety obligations to the underlying thread-safe collection. This eliminates the need for explicit synchronization in <tt class="calibre25">AttributeStore</tt>, reduces the lock scope to the duration of the <tt class="calibre25">Map</tt> access, and removes the risk that a future maintainer will undermine thread safety by forgetting to acquire the appropriate lock before accessing <tt class="calibre25">attributes</tt>.</p>
<p class="docText1">While shrinking <tt class="calibre25">synchronized</tt> blocks can improve scalability, a <tt class="calibre25">synchronized</tt> block can be <span class="docEmphasis">too</span> smalloperations that need to be atomic (such updating multiple variables that participate in an invariant) must be contained in a single <tt class="calibre25">synchronized</tt> <a name="iddle1367" class="calibre18" id="iddle1367"></a><a name="iddle1808" class="calibre18" id="iddle1808"></a><a name="iddle2469" class="calibre18" id="iddle2469"></a><a name="iddle2730" class="calibre18" id="iddle2730"></a><a name="iddle3073" class="calibre18" id="iddle3073"></a><a name="iddle3103" class="calibre18" id="iddle3103"></a><a name="iddle3146" class="calibre18" id="iddle3146"></a><a name="iddle3148" class="calibre18" id="iddle3148"></a><a name="iddle4348" class="calibre18" id="iddle4348"></a><a name="iddle4350" class="calibre18" id="iddle4350"></a><a name="iddle4434" class="calibre18" id="iddle4434"></a><a name="iddle4473" class="calibre18" id="iddle4473"></a><a name="iddle5059" class="calibre18" id="iddle5059"></a>block. And because the cost of synchronization is nonzero, breaking one <tt class="calibre25">synchronized</tt> block into multiple <tt class="calibre25">synchronized</tt> blocks (correctness permitting) at some point becomes counterproductive in terms of performance.<sup class="docFootnote"><a class="calibre2" href="#ch11fn09">[9]</a></sup> The ideal balance is of course platform-dependent, but in practice it makes sense to worry about the size of a <tt class="calibre25">synchronized</tt> block only when you can move "substantial" computation or blocking operations out of it.</p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn09" class="calibre18" id="ch11fn09">[9]</a></sup> If the JVM performs lock coarsening, it may undo the splitting of <tt class="calibre35">synchronized</tt> blocks anyway.</p></blockquote>
<a name="ch11lev2sec9" class="calibre18" id="ch11lev2sec9"></a>
<h4 id="title-IDARZQCN" class="docSection2Title">11.4.2. Reducing Lock Granularity</h4>
<p class="docText1">The other way to reduce the fraction of time that a lock is held (and therefore the likelihood that it will be contended) is to have threads ask for it less often. This can be accomplished by <span class="docEmphasis">lock splitting</span> and <span class="docEmphasis">lock striping</span>, which involve using separate locks to guard multiple independent state variables previously guarded by a single lock. These techniques reduce the granularity at which locking occurs, potentially allowing greater scalabilitybut using more locks also increases the risk of deadlock.</p>
<p class="docText1">As a thought experiment, imagine what would happen if there was <span class="docEmphasis">only one</span> lock for the entire application instead of a separate lock for each object. Then execution of all <tt class="calibre25">synchronized</tt> blocks, regardless of their lock, would be serialized. With many threads competing for the global lock, the chance that two threads want the lock at the same time increases, resulting in more contention. So if lock requests were instead distributed over a <span class="docEmphasis">larger</span> set of locks, there would be less contention. Fewer threads would be blocked waiting for locks, thus increasing scalability.</p>
<p class="docText1">If a lock guards more than one <span class="docEmphasis">independent</span> state variable, you may be able to improve scalability by splitting it into multiple locks that each guard different variables. This results in each lock being requested less often.</p>
<p class="docText1"><tt class="calibre25">ServerStatus</tt> in <a class="calibre2" href="#ch11list06">Listing 11.6</a> shows a portion of the monitoring interface for a database server that maintains the set of currently logged-on users and the set of currently executing queries. As a user logs on or off or query execution begins or ends, the <tt class="calibre25">ServerStatus</tt> object is updated by calling the appropriate <tt class="calibre25">add</tt> or <tt class="calibre25">remove</tt> method. The two types of information are completely independent; <tt class="calibre25">ServerStatus</tt> could even be split into two separate classes with no loss of functionality.</p>
<p class="docText1">Instead of guarding both <tt class="calibre25">users</tt> and <tt class="calibre25">queries</tt> with the <tt class="calibre25">ServerStatus</tt> lock, we can instead guard each with a separate lock, as shown in <a class="calibre2" href="#ch11list07">Listing 11.7</a>. After splitting the lock, each new finer-grained lock will see less locking traffic than the original coarser lock would have. (Delegating to a thread-safe <tt class="calibre25">Set</tt> implementation for <tt class="calibre25">users</tt> and <tt class="calibre25">queries</tt> instead of using explicit synchronization would implicitly provide lock splitting, as each <tt class="calibre25">Set</tt> would use a different lock to guard its state.)</p>
<p class="docText1">Splitting a lock into two offers the greatest possibility for improvement when the lock is experiencing moderate but not heavy contention. Splitting locks that are experiencing little contention yields little net improvement in performance or throughput, although it might increase the load threshold at which performance starts to degrade due to contention. Splitting locks experiencing moderate contention <a name="iddle2177" class="calibre18" id="iddle2177"></a><a name="iddle3149" class="calibre18" id="iddle3149"></a><a name="iddle4351" class="calibre18" id="iddle4351"></a><a name="iddle1380" class="calibre18" id="iddle1380"></a><a name="iddle1747" class="calibre18" id="iddle1747"></a><a name="iddle1748" class="calibre18" id="iddle1748"></a><a name="iddle2340" class="calibre18" id="iddle2340"></a><a name="iddle2341" class="calibre18" id="iddle2341"></a><a name="iddle2629" class="calibre18" id="iddle2629"></a><a name="iddle2653" class="calibre18" id="iddle2653"></a><a name="iddle2654" class="calibre18" id="iddle2654"></a><a name="iddle2655" class="calibre18" id="iddle2655"></a><a name="iddle2837" class="calibre18" id="iddle2837"></a><a name="iddle3114" class="calibre18" id="iddle3114"></a><a name="iddle3151" class="calibre18" id="iddle3151"></a><a name="iddle3154" class="calibre18" id="iddle3154"></a><a name="iddle3803" class="calibre18" id="iddle3803"></a><a name="iddle4068" class="calibre18" id="iddle4068"></a><a name="iddle4499" class="calibre18" id="iddle4499"></a><a name="iddle4500" class="calibre18" id="iddle4500"></a>might actually turn them into mostly uncontended locks, which is the most desirable outcome for both performance and scalability.</p>
<a name="ch11list06" class="calibre18" id="ch11list06"></a><h5 id="title-IDA1CRCN" class="docExampleTitle">Listing 11.6. Candidate for Lock Splitting.</h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<pre class="calibre30">@ThreadSafe
public class ServerStatus {
    @GuardedBy("this") public final Set&lt;String&gt; users;
    @GuardedBy("this") public final Set&lt;String&gt; queries;
    ...
    public synchronized void addUser(String u) { users.add(u); }
    public synchronized void addQuery(String q) { queries.add(q); }
    public synchronized void removeUser(String u) {
        users.remove(u);
    }
    public synchronized void removeQuery(String q) {
        queries.remove(q);
    }
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<a name="ch11list07" class="calibre18" id="ch11list07"></a><h5 id="title-IDAODRCN" class="docExampleTitle">Listing 11.7. <tt class="calibre33">ServerStatus</tt> Refactored to Use Split Locks.</h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<pre class="calibre30">@ThreadSafe
public class ServerStatus {
    @GuardedBy("users") public final Set&lt;String&gt; users;
    @GuardedBy("queries") public final Set&lt;String&gt; queries;
    ...
    public void addUser(String u) {
        <span class="docEmphStrong">synchronized</span>  (users) {
            users.add(u);
        }
    }

    public void addQuery(String q) {
        <span class="docEmphStrong">synchronized</span>  (queries) {
            queries.add(q);
        }
    }
    <span class="docEmphasis">// remove methods similarly refactored to use split locks</span>
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<a name="ch11lev2sec10" class="calibre18" id="ch11lev2sec10"></a>
<h4 id="title-IDAJERCN" class="docSection2Title">11.4.3. Lock Striping</h4>
<p class="docText1">Splitting a heavily contended lock into two is likely to result in two heavily contended locks. While this will produce a small scalability improvement by enabling two threads to execute concurrently instead of one, it still does not dramatically improve prospects for concurrency on a system with many processors. The lock splitting example in the <tt class="calibre25">ServerStatus</tt> classes does not offer any obvious opportunity for splitting the locks further.</p>
<p class="docText1">Lock splitting can sometimes be extended to partition locking on a variablesized set of independent objects, in which case it is called <span class="docEmphasis">lock striping</span>. For example, the implementation of <tt class="calibre25">ConcurrentHashMap</tt> uses an array of 16 locks, each of which guards 1/16 of the hash buckets; bucket <span class="docEmphasis">N</span> is guarded by lock <span class="docEmphasis">N</span> mod 16. Assuming the hash function provides reasonable spreading characteristics and keys are accessed uniformly, this should reduce the demand for any given lock by approximately a factor of 16. It is this technique that enables <tt class="calibre25">ConcurrentHashMap</tt> to support up to 16 concurrent writers. (The number of locks could be increased to provide even better concurrency under heavy access on high-processor-count systems, but the number of stripes should be increased beyond the default of 16 only when you have evidence that concurrent writers are generating enough contention to warrant raising the limit.)</p>
<p class="docText1">One of the downsides of lock striping is that locking the collection for exclusive access is more difficult and costly than with a single lock. Usually an operation can be performed by acquiring at most one lock, but occasionally you need to lock the entire collection, as when <tt class="calibre25">ConcurrentHashMap</tt> needs to expand the map and rehash the values into a larger set of buckets. This is typically done by acquiring all of the locks in the stripe set.<sup class="docFootnote"><a class="calibre2" href="#ch11fn10">[10]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn10" class="calibre18" id="ch11fn10">[10]</a></sup> The only way to acquire an arbitrary set of intrinsic locks is via recursion.</p></blockquote>
<p class="docText1"><tt class="calibre25">StripedMap</tt> in <a class="calibre2" href="#ch11list08">Listing 11.8</a> illustrates implementing a hash-based map using lock striping. There are <tt class="calibre25">N_LOCKS</tt> locks, each guarding a subset of the buckets. Most methods, like <tt class="calibre25">get</tt>, need acquire only a single bucket lock. Some methods may need to acquire all the locks but, as in the implementation for <tt class="calibre25">clear</tt>, may not need to acquire them all simultaneously.<sup class="docFootnote"><a class="calibre2" href="#ch11fn11">[11]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn11" class="calibre18" id="ch11fn11">[11]</a></sup> Clearing the <tt class="calibre35">Map</tt> in this way is not atomic, so there is not necessarily a time when the <tt class="calibre35">Striped-Map</tt> is actually empty if other threads are concurrently adding elements; making the operation atomic would require acquiring all the locks at once. However, for concurrent collections that clients typically cannot lock for exclusive access, the result of methods like <tt class="calibre35">size</tt> or <tt class="calibre35">isEmpty</tt> may be out of date by the time they return anyway, so this behavior, while perhaps somewhat surprising, is usually acceptable.</p></blockquote>
<a name="ch11lev2sec11" class="calibre18" id="ch11lev2sec11"></a>
<h4 id="title-IDALGRCN" class="docSection2Title">11.4.4. Avoiding Hot Fields</h4>
<p class="docText1">Lock splitting and lock striping can improve scalability because they enable different threads to operate on different data (or different portions of the same data structure) without interfering with each other. A program that would benefit from lock splitting necessarily exhibits contention for a <span class="docEmphasis">lock</span> more often than for the <span class="docEmphasis">data</span> <a name="iddle2186" class="calibre18" id="iddle2186"></a><a name="iddle1138" class="calibre18" id="iddle1138"></a><a name="iddle2342" class="calibre18" id="iddle2342"></a><a name="iddle2466" class="calibre18" id="iddle2466"></a><a name="iddle2656" class="calibre18" id="iddle2656"></a><a name="iddle2657" class="calibre18" id="iddle2657"></a><a name="iddle3094" class="calibre18" id="iddle3094"></a><a name="iddle3095" class="calibre18" id="iddle3095"></a><a name="iddle3259" class="calibre18" id="iddle3259"></a><a name="iddle3260" class="calibre18" id="iddle3260"></a><a name="iddle3526" class="calibre18" id="iddle3526"></a><a name="iddle3797" class="calibre18" id="iddle3797"></a><a name="iddle4075" class="calibre18" id="iddle4075"></a>guarded by that lock. If a lock guards two independent variables <span class="docEmphasis">X</span> and <span class="docEmphasis">Y</span>, and thread <span class="docEmphasis">A</span> wants to access <span class="docEmphasis">X</span> while <span class="docEmphasis">B</span> wants to access <span class="docEmphasis">Y</span> (as would be the case if one thread called <tt class="calibre25">addUser</tt> while another called <tt class="calibre25">addQuery</tt> in <tt class="calibre25">ServerStatus</tt>), then the two threads are not contending for any data, even though they are contending for a lock.</p>
<a name="ch11list08" class="calibre18" id="ch11list08"></a><h5 id="title-IDABMRCN" class="docExampleTitle">Listing 11.8. Hash-based Map Using Lock Striping.</h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<pre class="calibre30">@ThreadSafe
public class StripedMap {
    <span class="docEmphasis">// Synchronization policy: buckets[n] guarded by locks[n%N_LOCKS]</span>
    private static final int N_LOCKS = 16;
    private final Node[] buckets;
    private final Object[] locks;

    private static class Node { ... }

    public StripedMap(int numBuckets) {
        buckets = new Node[numBuckets];
        locks = new Object[N_LOCKS];
        for (int i = 0; i &lt; N_LOCKS; i++)
            locks[i] = new Object();
    }

    private final int hash(Object key) {
        return Math.abs(key.hashCode() % buckets.length);
    }

    public Object get(Object key) {
        int hash = hash(key);
        synchronized (locks[hash % N_LOCKS]) {
            for (Node m = buckets[hash]; m != null; m = m.next)
                if (m.key.equals(key))
                    return m.value;
        }
        return null;
    }

    public void clear() {
        for (int i = 0; i &lt; buckets.length; i++) {
            synchronized (locks[i % N_LOCKS]) {
                buckets[i] = null;
            }
        }
    }
    ...
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<p class="docText1">Lock granularity cannot be reduced when there are variables that are required for every operation. This is yet another area where raw performance and scalability are often at odds with each other; common optimizations such as caching frequently computed values can introduce "hot fields" that limit scalability.</p>
<p class="docText1">If you were implementing <tt class="calibre25">HashMap</tt>, you would have a choice of how <tt class="calibre25">size</tt> computes the number of entries in the <tt class="calibre25">Map</tt>. The simplest approach is to count the number of entries every time it is called. A common optimization is to update a separate counter as entries are added or removed; this slightly increases the cost of a <tt class="calibre25">put</tt> or <tt class="calibre25">remove</tt> operation to keep the counter up-to-date, but reduces the cost of the <tt class="calibre25">size</tt> operation from <span class="docEmphasis">O</span>(<span class="docEmphasis">n</span>) to <span class="docEmphasis">O</span>(1).</p>
<p class="docText1">Keeping a separate count to speed up operations like <tt class="calibre25">size</tt> and <tt class="calibre25">isEmpty</tt> works fine for a single-threaded or fully synchronized implementation, but makes it much harder to improve the scalability of the implementation because every operation that modifies the map must now update the shared counter. Even if you use lock striping for the hash chains, synchronizing access to the counter reintroduces the scalability problems of exclusive locking. What looked like a performance optimizationcaching the results of the <tt class="calibre25">size</tt> operationhas turned into a scalability liability. In this case, the counter is called a <span class="docEmphasis">hot field</span> because every mutative operation needs to access it.</p>
<p class="docText1"><tt class="calibre25">ConcurrentHashMap</tt> avoids this problem by having <tt class="calibre25">size</tt> enumerate the stripes and add up the number of elements in each stripe, instead of maintaining a global count. To avoid enumerating every element, <tt class="calibre25">ConcurrentHashMap</tt> maintains a separate count field for each stripe, also guarded by the stripe lock.<sup class="docFootnote"><a class="calibre2" href="#ch11fn12">[12]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn12" class="calibre18" id="ch11fn12">[12]</a></sup> If <tt class="calibre35">size</tt> is called frequently compared to mutative operations, striped data structures can optimize for this by caching the collection size in a <tt class="calibre35">volatile</tt> whenever <tt class="calibre35">size</tt> is called and invalidating the cache (setting it to -1) whenever the collection is modified. If the cached value is nonnegative on entry to <tt class="calibre35">size</tt>, it is accurate and can be returned; otherwise it is recomputed.</p></blockquote>
<a name="ch11lev2sec12" class="calibre18" id="ch11lev2sec12"></a>
<h4 id="title-IDAKORCN" class="docSection2Title">11.4.5. Alternatives to Exclusive Locks</h4>
<p class="docText1">A third technique for mitigating the effect of lock contention is to forego the use of exclusive locks in favor of a more concurrency-friendly means of managing shared state. These include using the concurrent collections, read-write locks, immutable objects and atomic variables.</p>
<p class="docText1"><tt class="calibre25">ReadWriteLock</tt> (see <a class="calibre2" href="ch13.html#ch13">Chapter 13</a>) enforces a multiple-reader, single-writer locking discipline: more than one reader can access the shared resource concurrently so long as none of them wants to modify it, but writers must acquire the lock excusively. For read-mostly data structures, <tt class="calibre25">ReadWriteLock</tt> can offer greater concurrency than exclusive locking; for read-only data structures, immutability can eliminate the need for locking entirely.</p>
<p class="docText1">Atomic variables (see <a class="calibre2" href="ch15.html#ch15">Chapter 15</a>) offer a means of reducing the cost of updating "hot fields" such as statistics counters, sequence generators, or the reference <a name="iddle1090" class="calibre18" id="iddle1090"></a><a name="iddle1091" class="calibre18" id="iddle1091"></a><a name="iddle1591" class="calibre18" id="iddle1591"></a><a name="iddle1713" class="calibre18" id="iddle1713"></a><a name="iddle2684" class="calibre18" id="iddle2684"></a><a name="iddle2869" class="calibre18" id="iddle2869"></a><a name="iddle3078" class="calibre18" id="iddle3078"></a><a name="iddle3079" class="calibre18" id="iddle3079"></a><a name="iddle3187" class="calibre18" id="iddle3187"></a><a name="iddle3251" class="calibre18" id="iddle3251"></a><a name="iddle3257" class="calibre18" id="iddle3257"></a><a name="iddle3258" class="calibre18" id="iddle3258"></a><a name="iddle3473" class="calibre18" id="iddle3473"></a><a name="iddle3474" class="calibre18" id="iddle3474"></a><a name="iddle3475" class="calibre18" id="iddle3475"></a><a name="iddle3692" class="calibre18" id="iddle3692"></a><a name="iddle3693" class="calibre18" id="iddle3693"></a><a name="iddle4064" class="calibre18" id="iddle4064"></a><a name="iddle4770" class="calibre18" id="iddle4770"></a><a name="iddle4943" class="calibre18" id="iddle4943"></a><a name="iddle4944" class="calibre18" id="iddle4944"></a><a name="iddle4949" class="calibre18" id="iddle4949"></a><a name="iddle4950" class="calibre18" id="iddle4950"></a><a name="iddle5111" class="calibre18" id="iddle5111"></a>to the first node in a linked data structure. (We used <tt class="calibre25">AtomicLong</tt> to maintain the hit counter in the servlet examples in <a class="calibre2" href="ch02_split_000.html#ch02">Chapter 2</a>.) The atomic variable classes provide very fine-grained (and thereforemore scalable) atomic operations on integers or object references, and are implemented using low-level concurrency primitives (such as compare-and-swap) provided by most modern processors. If your class has a small number of hot fields that do not participate in invariants with other variables, replacing them with atomic variables may improve scalability. (Changing your algorithm to have fewer hot fields might improve scalability even moreatomic variables reduce the cost of updating hot fields, but they don't eliminate it.)</p>
<a name="ch11lev2sec13" class="calibre18" id="ch11lev2sec13"></a>
<h4 id="title-IDACZRCN" class="docSection2Title">11.4.6. Monitoring CPU Utilization</h4>
<p class="docText1">When testing for scalability, the goal is usually to keep the processors fully utilized. Tools like <tt class="calibre25">vmstat</tt> and <tt class="calibre25">mpstat</tt> on Unix systems or <tt class="calibre25">perfmon</tt> on Windows systems can tell you just how "hot" the processors are running.</p>
<p class="docText1">If the CPUs are asymmetrically utilized (some CPUs are running hot but others are not) your first goal should be to find increased parallelism in your program. Asymmetric utilization indicates that most of the computation is going on in a small set of threads, and your application will not be able to take advantage of additional processors.</p>
<p class="docText1">If the CPUs are not fully utilized, you need to figure out why. There are several likely causes:</p>
<blockquote class="calibre19"><p class="calibre21"></p><p class="docText1"><span class="docEmphStrong">Insufficent load.</span> It may be that the application being tested is just not subjected to enough load. You can test for this by increasing the load and measuring changes in utilization, response time, or service time. Generating enough load to saturate an application can require substantial computer power; the problem may be that the client systems, not the system being tested, are running at capacity.</p><p class="calibre21"></p><p class="docText1"><span class="docEmphStrong">I/O-bound.</span> You can determine whether an application is disk-bound using <tt class="calibre25">iostat</tt> or <tt class="calibre25">perfmon</tt>, and whether it is bandwidth-limited by monitoring traffic levels on your network.</p><p class="calibre21"></p><p class="docText1"><span class="docEmphStrong">Externally bound.</span> If your application depends on external services such as a database or web service, the bottleneck may not be in your code. You can test for this by using a profiler or database administration tools to determine how much time is being spent waiting for answers from the external service.</p><p class="calibre21"></p><p class="docText1"><span class="docEmphStrong">Lock contention.</span> Profiling tools can tell you how much lock contention your application is experiencing and which locks are "hot". You can often get the same information without a profiler through random sampling, triggering a few thread dumps and looking for threads contending for locks. If a thread is blocked waiting for a lock, the appropriate stack frame in the thread dump indicates "waiting to lock monitor . . . " Locks that are mostly uncontended rarely show up in a thread dump; a heavily contended lock will almost always have at least one thread waiting to acquire it and so will frequently appear in thread dumps.</p></blockquote>
<p class="docText1"><a name="iddle1074" class="calibre18" id="iddle1074"></a><a name="iddle1075" class="calibre18" id="iddle1075"></a><a name="iddle1473" class="calibre18" id="iddle1473"></a><a name="iddle3355" class="calibre18" id="iddle3355"></a><a name="iddle3356" class="calibre18" id="iddle3356"></a><a name="iddle3358" class="calibre18" id="iddle3358"></a><a name="iddle3527" class="calibre18" id="iddle3527"></a><a name="iddle3607" class="calibre18" id="iddle3607"></a><a name="iddle3608" class="calibre18" id="iddle3608"></a><a name="iddle3610" class="calibre18" id="iddle3610"></a><a name="iddle4076" class="calibre18" id="iddle4076"></a><a name="iddle5113" class="calibre18" id="iddle5113"></a>If your application is keeping the CPUs sufficiently hot, you can use monitoring tools to infer whether it would benefit from additional CPUs. A program with only four threads may be able to keep a 4-way system fully utilized, but is unlikely to see a performance boost if moved to an 8-way system since there would need to be waiting runnable threads to take advantage of the additional processors. (You may also be able to reconfigure the program to divide its workload over more threads, such as adjusting a thread pool size.) One of the columns reported by <tt class="calibre25">vmstat</tt> is the number of threads that are runnable but not currently running because a CPU is not available; if CPU utilization is high and there are always runnable threads waiting for a CPU, your application would probably benefit from more processors.</p>
<a name="ch11lev2sec14" class="calibre18" id="ch11lev2sec14"></a>
<h4 id="title-IDAQ5RCN" class="docSection2Title">11.4.7. Just Say No to Object Pooling</h4>
<p class="docText1">In early JVM versions, object allocation and garbage collection were slow,<sup class="docFootnote"><a class="calibre2" href="#ch11fn13">[13]</a></sup> but their performance has improved substantially since then. In fact, allocation in Java is now faster than <tt class="calibre25">malloc</tt> is in C: the common code path for <tt class="calibre25">new Object</tt> in HotSpot 1.4.x and 5.0 is approximately ten machine instructions.</p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn13" class="calibre18" id="ch11fn13">[13]</a></sup> As was everything elsesynchronization, graphics, JVM startup, reflectionpredictably so in the first version of an experimental technology.</p></blockquote>
<p class="docText1">To work around "slow" object lifecycles, many developers turned to object pooling, where objects are recycled instead of being garbage collected and allocated anew when needed. Even taking into account its reduced garbage collection overhead, object pooling has been shown to be a performance loss<sup class="docFootnote"><a class="calibre2" href="#ch11fn14">[14]</a></sup> for all but the most expensive objects (and a serious loss for light- and medium-weight objects) in single-threaded programs (<a class="calibre2" href="bib01.html#biblio01_009">Click, 2005</a>).</p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn14" class="calibre18" id="ch11fn14">[14]</a></sup> In addition to being a loss in terms of CPU cycles, object pooling has a number of other problems, among them the challenge of setting pool sizes correctly (too small, and pooling has no effect; too large, and it puts pressure on the garbage collector, retaining memory that could be used more effectively for something else); the risk that an object will not be properly reset to its newly allocated state, introducing subtle bugs; the risk that a thread will return an object to the pool but continue using it; and that it makes more work for generational garbage collectors by encouraging a pattern of old-to-young references.</p></blockquote>
<p class="docText1">In concurrent applications, pooling fares even worse. When threads allocate new objects, very little inter-thread coordination is required, as allocators typically use thread-local allocation blocks to eliminate most synchronization on heap data structures. But if those threads instead request an object from a pool, some synchronization is necessary to coordinate access to the pool data structure, creating the possibility that a thread will block. Because blocking a thread due to lock contention is hundreds of times more expensive than an allocation, even a small amount of pool-induced contention would be a scalability bottleneck. (Even an uncontended synchronization is usually more expensive than allocating an object.) This is yet another technique intended as a performance optimization but that turned into a scalability hazard. Pooling has its uses,<sup class="docFootnote"><a class="calibre2" href="#ch11fn15">[15]</a></sup> but is of limited utility as a performance optimization.</p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn15" class="calibre18" id="ch11fn15">[15]</a></sup> In constrained environments, such as some J2ME or RTSJ targets, object pooling may still be required for effective memory management or to manage responsiveness.</p></blockquote>
<a name="ch11sb08" class="calibre18" id="ch11sb08"></a><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<p class="docText1"><a name="iddle1071" class="calibre18" id="iddle1071"></a><a name="iddle1072" class="calibre18" id="iddle1072"></a><a name="iddle1481" class="calibre18" id="iddle1481"></a><a name="iddle2502" class="calibre18" id="iddle2502"></a><a name="iddle2625" class="calibre18" id="iddle2625"></a><a name="iddle3180" class="calibre18" id="iddle3180"></a><a name="iddle4063" class="calibre18" id="iddle4063"></a><a name="iddle4536" class="calibre18" id="iddle4536"></a>Allocating objects is usually cheaper than synchronizing.</p>
</td></tr></table></p><p class="calibre1"> </p>

<p class="calibre1"> </p>

</div>

{% endraw %}

