---
layout: page
title: "Java Concurrency in Practice"
prev: ch11lev1sec5.html
next: ch11lev1sec7.html
book_path: books/brian-goetz-tim-peierls-joshua-bloch-joseph-bowbeer-david-holmes-doug-lea-java-concurrency-in-practice--_oeb/
---
{% include JB/setup %}
{% raw %}
<div>


<a name="ch11lev1sec6" class="calibre18" id="ch11lev1sec6"></a>
<h3 id="title-IDAIWQCN" class="docSection1Title">11.6. Reducing Context Switch Overhead</h3>
<p class="docText1"><a name="iddle1615" class="calibre18" id="iddle1615"></a><a name="iddle1616" class="calibre18" id="iddle1616"></a><a name="iddle1618" class="calibre18" id="iddle1618"></a><a name="iddle2674" class="calibre18" id="iddle2674"></a><a name="iddle2675" class="calibre18" id="iddle2675"></a><a name="iddle4188" class="calibre18" id="iddle4188"></a><a name="iddle4189" class="calibre18" id="iddle4189"></a>Many tasks involve operations that may block; transitioning between the running and blocked states entails a context switch. One source of blocking in server applications is generating log messages in the course of processing requests; to illustrate how throughput can be improved by reducing context switches, we'll analyze the scheduling behavior of two logging approaches.</p>
<p class="docText1">Most logging frameworks are thin wrappers around <tt class="calibre25">println</tt>; when you have something to log, just write it out right then and there. Another approach was shown in <tt class="calibre25">LogWriter</tt> on page 152: the logging is performed in a dedicated background thread instead of by the requesting thread. From the developer's perspective, both approaches are roughly equivalent. But there may be a difference in performance, depending on the volume of logging activity, how many threads are doing logging, and other factors such as the cost of context switching.<sup class="docFootnote"><a class="calibre2" href="#ch11fn16">[16]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn16" class="calibre18" id="ch11fn16">[16]</a></sup> Building a logger that moves the I/O to another thread may improve performance, but it also introduces a number of design complications, such as interruption (what happens if a thread blocked in a logging operation is interrupted?), service guarantees (does the logger guarantee that a successfully queued log message will be logged prior to service shutdown?), saturation policy (what happens when the producers log messages faster than the logger thread can handle them?), and service lifecycle (how do we shut down the logger, and how do we communicate the service state to producers?).</p></blockquote>
<p class="docText1">The service time for a logging operation includes whatever computation is associated with the I/O stream classes; if the I/O operation blocks, it also includes the duration for which the thread is blocked. The operating system will deschedule the blocked thread until the I/O completes, and probably a little longer. When the I/O completes, other threads are probably active and will be allowed to finish out their scheduling quanta, and threads may already be waiting ahead of us on <a name="iddle2615" class="calibre18" id="iddle2615"></a><a name="iddle3414" class="calibre18" id="iddle3414"></a><a name="iddle3461" class="calibre18" id="iddle3461"></a><a name="iddle3462" class="calibre18" id="iddle3462"></a><a name="iddle3724" class="calibre18" id="iddle3724"></a><a name="iddle4190" class="calibre18" id="iddle4190"></a>the scheduling queuefurther adding to service time. Alternatively, if multiple threads are logging simultaneously, theremay be contention for the output stream lock, in which case the result is the same as with blocking I/Othe thread blocks waiting for the lock and gets switched out. Inline logging involves I/O and locking, which can lead to increased context switching and therefore increased service times.</p>
<p class="docText1">Increasing request service time is undesirable for several reasons. First, service time affects quality of service: longer service times mean someone is waiting longer for a result. But more significantly, longer service times in this case mean more lock contention. The "get in, get out" principle of <a class="calibre2" href="ch11lev1sec4.html#ch11lev2sec8">Section 11.4.1</a> tells us that we should hold locks as briefly as possible, because the longer a lock is held, the more likely that lock will be contended. If a thread blocks waiting for I/O while holding a lock, another thread is more likely to want the lock while the first thread is holding it. Concurrent systems perform much better when most lock acquisitions are uncontended, because contended lock acquisition means more context switches. A coding style that encourages more context switches thus yields lower overall throughput.</p>
<p class="docText1">Moving the I/O out of the request-processing thread is likely to shorten the mean service time for request processing. Threads calling <tt class="calibre25">log</tt> no longer block waiting for the output stream lock or for I/O to complete; they need only queue the message and can then return to their task. On the other hand, we've introduced the possibility of contention for the message queue, but the <tt class="calibre25">put</tt> operation is lighter-weight than the logging I/O (which might require system calls) and so is less likely to block in actual use (as long as the queue is not full). Because the request thread is now less likely to block, it is less likely to be context-switched out in the middle of a request. What we've done is turned a complicated and uncertain code path involving I/O and possible lock contention into a straight-line code path.</p>
<p class="docText1">To some extent, we are just moving the work around, moving the I/O to a thread where its cost isn't perceived by the user (which may in itself be a win). But by moving <span class="docEmphasis">all</span> the logging I/O to a single thread, we also eliminate the chance of contention for the output stream and thus eliminate a source of blocking. This improves overall throughput because fewer resources are consumed in scheduling, context switching, and lock management.</p>
<p class="docText1">Moving the I/O from many request-processing threads to a single logger thread is similar to the difference between a bucket brigade and a collection of individuals fighting a fire. In the "hundred guys running around with buckets" approach, you have a greater chance of contention at the water source and at the fire (resulting in overall less water delivered to the fire), plus greater inefficiency because each worker is continuously switching modes (filling, running, dumping, running, etc.). In the bucket-brigade approach, the flow of water from the source to the burning building is constant, less energy is expended transporting the water to the fire, and each worker focuses on doing one job continuously. Just as interruptions are disruptive and productivity-reducing to humans, blocking and context switching are disruptive to threads.</p>
<a href="21021536.html" class="calibre2"><img src="pixel.jpg" alt="" border="0" class="calibre26"/></a>
<p class="calibre3">Â </p>

</div>

{% endraw %}

