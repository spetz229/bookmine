---
layout: page
title: "Java Concurrency in Practice"
prev: ch11lev1sec2.html
next: ch11lev1sec4.html
book_path: books/brian-goetz-tim-peierls-joshua-bloch-joseph-bowbeer-david-holmes-doug-lea-java-concurrency-in-practice--_oeb/
---
{% include JB/setup %}
{% raw %}
<div>


<a name="ch11lev1sec3" class="calibre18" id="ch11lev1sec3"></a>
<h3 id="647011-884" class="docSection1Title">11.3. Costs Introduced by Threads</h3>
<p class="docText1">Single-threaded programs incur neither scheduling nor synchronization overhead, and need not use locks to preserve the consistency of data structures. Scheduling and interthread coordination have performance costs; for threads to offer a performance improvement, the performance benefits of parallelization must outweigh the costs introduced by concurrency.</p>
<a name="ch11lev2sec5" class="calibre18" id="ch11lev2sec5"></a>
<h4 id="title-IDA211XE" class="docSection2Title">11.3.1. Context Switching</h4>
<p class="docText1">If the main thread is the only schedulable thread, it will almost never be scheduled out. On the other hand, if there are more runnable threads than CPUs, eventually the OS will preempt one thread so that another can use the CPU. This causes a <span class="docEmphasis">context switch</span>, which requires saving the execution context of the currently running thread and restoring the execution context of the newly scheduled thread.</p>
<p class="docText1">Context switches are not free; thread scheduling requires manipulating shared data structures in the OS and JVM. The OS and JVMuse the same CPUs your program does; more CPU time spent in JVM and OS code means less is available for your program. But OS and JVM activity is not the only cost of context switches. When a new thread is switched in, the data it needs is unlikely to be in the local processor cache, so a context switch causes a flurry of cache misses, and thus threads run a little more slowly when they are first scheduled. This is one of the reasons that schedulers give each runnable thread a certain minimum time quantum even when many other threads are waiting: it amortizes the cost of the context switch and its consequences over more uninterrupted execution time, improving overall throughput (at some cost to responsiveness).</p>
<p class="docText1"></p><a name="ch11list02" class="calibre18" id="ch11list02"></a><h5 id="title-IDAQ21XE" class="docExampleTitle">Listing 11.2. Synchronization that has No Effect. <span class="docEmphasis">Don't Do this.</span></h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<img border="0" alt="" id="195131084199" src="face.jpg" class="calibre29"/>
<pre class="calibre30">synchronized (new Object()) {
    <span class="docEmphasis">// do something</span>
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<p class="docText1"><a name="iddle1088" class="calibre18" id="iddle1088"></a><a name="iddle1195" class="calibre18" id="iddle1195"></a><a name="iddle1212" class="calibre18" id="iddle1212"></a><a name="iddle1281" class="calibre18" id="iddle1281"></a><a name="iddle1282" class="calibre18" id="iddle1282"></a><a name="iddle1604" class="calibre18" id="iddle1604"></a><a name="iddle2047" class="calibre18" id="iddle2047"></a><a name="iddle2334" class="calibre18" id="iddle2334"></a><a name="iddle2916" class="calibre18" id="iddle2916"></a><a name="iddle2925" class="calibre18" id="iddle2925"></a><a name="iddle3194" class="calibre18" id="iddle3194"></a><a name="iddle3206" class="calibre18" id="iddle3206"></a><a name="iddle3207" class="calibre18" id="iddle3207"></a><a name="iddle3476" class="calibre18" id="iddle3476"></a><a name="iddle3509" class="calibre18" id="iddle3509"></a><a name="iddle3854" class="calibre18" id="iddle3854"></a><a name="iddle3855" class="calibre18" id="iddle3855"></a><a name="iddle4546" class="calibre18" id="iddle4546"></a><a name="iddle4554" class="calibre18" id="iddle4554"></a><a name="iddle4558" class="calibre18" id="iddle4558"></a><a name="iddle4559" class="calibre18" id="iddle4559"></a><a name="iddle4579" class="calibre18" id="iddle4579"></a><a name="iddle4946" class="calibre18" id="iddle4946"></a><a name="iddle5006" class="calibre18" id="iddle5006"></a><a name="iddle5007" class="calibre18" id="iddle5007"></a><a name="iddle5112" class="calibre18" id="iddle5112"></a>When a thread blocks because it is waiting for a contended lock, the JVM usually suspends the thread and allows it to be switched out. If threads block frequently, they will be unable to use their full scheduling quantum. A program that does more blocking (blocking I/O, waiting for contended locks, or waiting on condition variables) incurs more context switches than one that is CPU-bound, increasing scheduling overhead and reducing throughput. (Nonblocking algorithms can also help reduce context switches; see <a class="calibre2" href="ch15.html#ch15">Chapter 15</a>.)</p>
<p class="docText1">The actual cost of context switching varies across platforms, but a good rule of thumb is that a context switch costs the equivalent of 5,000 to 10,000 clock cycles, or several microseconds on most current processors.</p>
<p class="docText1">The <tt class="calibre25">vmstat</tt> command on Unix systems and the <tt class="calibre25">perfmon</tt> tool on Windows systems report the number of context switches and the percentage of time spent in the kernel. High kernel usage (over 10%) often indicates heavy scheduling activity, which may be caused by blocking due to I/O or lock contention.</p>
<a name="ch11lev2sec6" class="calibre18" id="ch11lev2sec6"></a>
<h4 id="title-IDAXGIGN" class="docSection2Title">11.3.2. Memory Synchronization</h4>
<p class="docText1">The performance cost of synchronization comes from several sources. The visibility guarantees provided by <tt class="calibre25">synchronized</tt> and <tt class="calibre25">volatile</tt> may entail using special instructions called <span class="docEmphasis">memory barriers</span> that can flush or invalidate caches, flush hardware write buffers, and stall execution pipelines. Memory barriers may also have indirect performance consequences because they inhibit other compiler optimizations; most operations cannot be reordered with memory barriers.</p>
<p class="docText1">When assessing the performance impact of synchronization, it is important to distinguish between <span class="docEmphasis">contended</span> and <span class="docEmphasis">uncontended</span> synchronization. The <tt class="calibre25">synchronized</tt> mechanism is optimized for the uncontended case (<tt class="calibre25">volatile</tt> is always uncontended), and at this writing, the performance cost of a "fast-path" uncontended synchronization ranges from 20 to 250 clock cycles for most systems. While this is certainly not zero, the effect of needed, uncontended synchronization is rarely significant in overall application performance, and the alternative involves compromising safety and potentially signing yourself (or your successor) up for some very painful bug hunting later.</p>
<p class="docText1">Modern JVMs can reduce the cost of incidental synchronization by optimizing away locking that can be proven never to contend. If a lock object is accessible only to the current thread, the JVM is permitted to optimize away a lock acquisition because there is no way another thread could synchronize on the same lock. For example, the lock acquisition in <a class="calibre2" href="#ch11list02">Listing 11.2</a> can always be eliminated by the JVM.</p>
<p class="docText1">More sophisticated JVMs can use <span class="docEmphasis">escape analysis</span> to identify when a local object reference is never published to the heap and is therefore thread-local. In <tt class="calibre25">getStoogeNames</tt> <a name="iddle1057" class="calibre18" id="iddle1057"></a><a name="iddle1182" class="calibre18" id="iddle1182"></a><a name="iddle1183" class="calibre18" id="iddle1183"></a><a name="iddle1366" class="calibre18" id="iddle1366"></a><a name="iddle1995" class="calibre18" id="iddle1995"></a><a name="iddle1996" class="calibre18" id="iddle1996"></a><a name="iddle2550" class="calibre18" id="iddle2550"></a><a name="iddle2560" class="calibre18" id="iddle2560"></a><a name="iddle2561" class="calibre18" id="iddle2561"></a><a name="iddle3071" class="calibre18" id="iddle3071"></a><a name="iddle3090" class="calibre18" id="iddle3090"></a><a name="iddle3304" class="calibre18" id="iddle3304"></a><a name="iddle3386" class="calibre18" id="iddle3386"></a><a name="iddle3387" class="calibre18" id="iddle3387"></a>in <a class="calibre2" href="#ch11list03">Listing 11.3</a>, the only reference to the <tt class="calibre25">List</tt> is the local variable <tt class="calibre25">stooges</tt>, and stack-confined variables are automatically thread-local. A naive execution of <tt class="calibre25">getStoogeNames</tt> would acquire and release the lock on the <tt class="calibre25">Vector</tt> four times, once for each call to <tt class="calibre25">add</tt> or <tt class="calibre25">toString</tt>. However, a smart runtime compiler can inline these calls and then see that <tt class="calibre25">stooges</tt> and its internal state never escape, and therefore that all four lock acquisitions can be eliminated.<sup class="docFootnote"><a class="calibre2" href="#ch11fn04">[4]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn04" class="calibre18" id="ch11fn04">[4]</a></sup> This compiler optimization, called <span class="docEmphasis">lock elision</span>, is performed by the IBM JVM and is expected in HotSpot as of Java 7.</p></blockquote>
<a name="ch11list03" class="calibre18" id="ch11list03"></a><h5 id="title-IDAWVQCN" class="docExampleTitle">Listing 11.3. Candidate for Lock Elision.</h5><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<pre class="calibre30">public String getStoogeNames() {
    List&lt;String&gt; stooges = new Vector&lt;String&gt;();
    stooges.add("Moe");
    stooges.add("Larry");
    stooges.add("Curly");
    return stooges.toString();
}
</pre><br class="calibre11"/>
</td></tr></table></p>
<p class="docText1">Even without escape analysis, compilers can also perform <span class="docEmphasis">lock coarsening</span>, the merging of adjacent <tt class="calibre25">synchronized</tt> blocks using the same lock. For <tt class="calibre25">getStooge-Names</tt>, a JVM that performs lock coarsening might combine the three calls to <tt class="calibre25">add</tt> and the call to <tt class="calibre25">toString</tt> into a single lock acquisition and release, using heuristics on the relative cost of synchronization versus the instructions inside the <tt class="calibre25">synchronized</tt> block.<sup class="docFootnote"><a class="calibre2" href="#ch11fn05">[5]</a></sup> Not only does this reduce the synchronization overhead, but it also gives the optimizer a much larger block to work with, likely enabling other optimizations.</p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn05" class="calibre18" id="ch11fn05">[5]</a></sup> A smart dynamic compiler can figure out that this method always returns the same string, and after the first execution recompile <tt class="calibre35">getStoogeNames</tt> to simply return the value returned by the first execution.</p></blockquote>
<a name="ch11sb05" class="calibre18" id="ch11sb05"></a><p class="calibre21"><table cellspacing="0" width="90%" border="1" cellpadding="5" class="calibre5"><tr class="calibre6"><td class="calibre28">
<p class="docText1">Don't worry excessively about the cost of uncontended synchronization. The basic mechanism is already quite fast, and JVMs can perform additional optimizations that further reduce or eliminate the cost. Instead, focus optimization efforts on areas where lock contention actually occurs.</p>
</td></tr></table></p><p class="calibre1"> </p>
<p class="docText1">Synchronization by one thread can also affect the performance of other threads. Synchronization creates traffic on the shared memory bus; this bus has a limited bandwidth and is shared across all processors. If threads must compete for synchronization bandwidth, all threads using synchronization will suffer.<sup class="docFootnote"><a class="calibre2" href="#ch11fn06">[6]</a></sup></p><blockquote class="calibre19"><p class="docFootnote1"><sup class="calibre27"><a name="ch11fn06" class="calibre18" id="ch11fn06">[6]</a></sup> This aspect is sometimes used to argue against the use of nonblocking algorithms without some sort of backoff, because under heavy contention, nonblocking algorithms generate more synchronization traffic than lock-based ones. See <a class="calibre2" href="ch15.html#ch15">Chapter 15</a>.</p></blockquote>
<a name="ch11lev2sec7" class="calibre18" id="ch11lev2sec7"></a>
<h4 id="title-IDAYXQCN" class="docSection2Title">11.3.3. Blocking</h4>
<p class="docText1"><a name="iddle1227" class="calibre18" id="iddle1227"></a><a name="iddle1233" class="calibre18" id="iddle1233"></a><a name="iddle1593" class="calibre18" id="iddle1593"></a><a name="iddle1594" class="calibre18" id="iddle1594"></a><a name="iddle2576" class="calibre18" id="iddle2576"></a><a name="iddle3009" class="calibre18" id="iddle3009"></a><a name="iddle3010" class="calibre18" id="iddle3010"></a><a name="iddle3082" class="calibre18" id="iddle3082"></a><a name="iddle3083" class="calibre18" id="iddle3083"></a><a name="iddle3388" class="calibre18" id="iddle3388"></a><a name="iddle3516" class="calibre18" id="iddle3516"></a><a name="iddle4065" class="calibre18" id="iddle4065"></a><a name="iddle4066" class="calibre18" id="iddle4066"></a><a name="iddle4341" class="calibre18" id="iddle4341"></a><a name="iddle4342" class="calibre18" id="iddle4342"></a><a name="iddle4510" class="calibre18" id="iddle4510"></a><a name="iddle4511" class="calibre18" id="iddle4511"></a><a name="iddle4825" class="calibre18" id="iddle4825"></a><a name="iddle4826" class="calibre18" id="iddle4826"></a><a name="iddle5142" class="calibre18" id="iddle5142"></a>Uncontended synchronization can be handled entirely within the JVM (<a class="calibre2" href="bib01.html#biblio01_002">Bacon et al., 1998</a>); contended synchronization may require OS activity, which adds to the cost. When locking is contended, the losing thread(s) must block. The JVM can implement blocking either via <span class="docEmphasis">spin-waiting</span> (repeatedly trying to acquire the lock until it succeeds) or by <span class="docEmphasis">suspending</span> the blocked thread through the operating system. Which is more efficient depends on the relationship between context switch overhead and the time until the lock becomes available; spin-waiting is preferable for short waits and suspension is preferable for long waits. Some JVMs choose between the two adaptively based on profiling data of past wait times, but most just suspend threads waiting for a lock.</p>
<p class="docText1">Suspending a thread because it could not get a lock, or because it blocked on a condition wait or blocking I/O operation, entails two additional context switches and all the attendant OS and cache activity: the blocked thread is switched out before its quantum has expired, and is then switched back in later after the lock or other resource becomes available. (Blocking due to lock contention also has a cost for the thread holding the lock: when it releases the lock, it must then ask the OS to resume the blocked thread.)</p>

<p class="calibre1"> </p>

</div>

{% endraw %}

